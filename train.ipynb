{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "WARNING:tensorflow:From <ipython-input-1-f00e0d4494b6>:52: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "Vocabulary Size: 18758\n",
      "Train/Dev split: 9596/1066\n",
      "WARNING:tensorflow:From D:\\New\\ML Group\\tensorflow2\\cnn-text-classification-tf-master\\text_cnn.py:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to D:\\New\\ML Group\\tensorflow2\\cnn-text-classification-tf-master\\runs\\1547294011\n",
      "\n",
      "2019-01-12T17:23:32.511385: step 1, loss 3.3726, acc 0.546875\n",
      "2019-01-12T17:23:32.650979: step 2, loss 2.94147, acc 0.40625\n",
      "2019-01-12T17:23:32.791668: step 3, loss 2.21512, acc 0.453125\n",
      "2019-01-12T17:23:32.928305: step 4, loss 2.45135, acc 0.484375\n",
      "2019-01-12T17:23:33.072849: step 5, loss 2.32718, acc 0.53125\n",
      "2019-01-12T17:23:33.213507: step 6, loss 2.15334, acc 0.59375\n",
      "2019-01-12T17:23:33.352103: step 7, loss 1.87596, acc 0.53125\n",
      "2019-01-12T17:23:33.500705: step 8, loss 1.86592, acc 0.515625\n",
      "2019-01-12T17:23:33.638371: step 9, loss 1.53352, acc 0.5625\n",
      "2019-01-12T17:23:33.776965: step 10, loss 1.91869, acc 0.546875\n",
      "2019-01-12T17:23:33.914598: step 11, loss 2.20831, acc 0.4375\n",
      "2019-01-12T17:23:34.050284: step 12, loss 1.60432, acc 0.578125\n",
      "2019-01-12T17:23:34.200832: step 13, loss 2.24446, acc 0.453125\n",
      "2019-01-12T17:23:34.341456: step 14, loss 1.55454, acc 0.578125\n",
      "2019-01-12T17:23:34.481081: step 15, loss 2.42815, acc 0.40625\n",
      "2019-01-12T17:23:34.623700: step 16, loss 2.02173, acc 0.515625\n",
      "2019-01-12T17:23:34.758374: step 17, loss 1.91628, acc 0.515625\n",
      "2019-01-12T17:23:34.897036: step 18, loss 1.75652, acc 0.59375\n",
      "2019-01-12T17:23:35.033672: step 19, loss 2.51699, acc 0.3125\n",
      "2019-01-12T17:23:35.169308: step 20, loss 2.50068, acc 0.375\n",
      "2019-01-12T17:23:35.308866: step 21, loss 1.95767, acc 0.546875\n",
      "2019-01-12T17:23:35.451485: step 22, loss 1.41324, acc 0.578125\n",
      "2019-01-12T17:23:35.595138: step 23, loss 1.8093, acc 0.46875\n",
      "2019-01-12T17:23:35.747692: step 24, loss 2.31842, acc 0.515625\n",
      "2019-01-12T17:23:35.895300: step 25, loss 1.68149, acc 0.546875\n",
      "2019-01-12T17:23:36.037917: step 26, loss 2.31728, acc 0.5\n",
      "2019-01-12T17:23:36.174550: step 27, loss 1.81555, acc 0.546875\n",
      "2019-01-12T17:23:36.319166: step 28, loss 2.25627, acc 0.421875\n",
      "2019-01-12T17:23:36.459788: step 29, loss 1.52241, acc 0.46875\n",
      "2019-01-12T17:23:36.596457: step 30, loss 1.6515, acc 0.546875\n",
      "2019-01-12T17:23:36.735052: step 31, loss 2.09558, acc 0.453125\n",
      "2019-01-12T17:23:36.885715: step 32, loss 1.62261, acc 0.59375\n",
      "2019-01-12T17:23:37.030301: step 33, loss 1.43086, acc 0.546875\n",
      "2019-01-12T17:23:37.168962: step 34, loss 1.56329, acc 0.625\n",
      "2019-01-12T17:23:37.310511: step 35, loss 2.02584, acc 0.46875\n",
      "2019-01-12T17:23:37.454128: step 36, loss 1.44591, acc 0.546875\n",
      "2019-01-12T17:23:37.602797: step 37, loss 1.66701, acc 0.546875\n",
      "2019-01-12T17:23:37.749406: step 38, loss 2.53625, acc 0.4375\n",
      "2019-01-12T17:23:37.897941: step 39, loss 1.80625, acc 0.453125\n",
      "2019-01-12T17:23:38.064497: step 40, loss 1.86411, acc 0.5\n",
      "2019-01-12T17:23:38.228060: step 41, loss 1.4761, acc 0.5\n",
      "2019-01-12T17:23:38.398601: step 42, loss 1.98455, acc 0.4375\n",
      "2019-01-12T17:23:38.544212: step 43, loss 1.50228, acc 0.5625\n",
      "2019-01-12T17:23:38.685834: step 44, loss 1.25959, acc 0.609375\n",
      "2019-01-12T17:23:38.824463: step 45, loss 1.38297, acc 0.546875\n",
      "2019-01-12T17:23:38.960100: step 46, loss 1.53762, acc 0.5625\n",
      "2019-01-12T17:23:39.103716: step 47, loss 1.61889, acc 0.515625\n",
      "2019-01-12T17:23:39.275258: step 48, loss 1.43329, acc 0.59375\n",
      "2019-01-12T17:23:39.452816: step 49, loss 1.44256, acc 0.53125\n",
      "2019-01-12T17:23:39.621397: step 50, loss 1.45513, acc 0.5625\n",
      "2019-01-12T17:23:39.781904: step 51, loss 1.84486, acc 0.484375\n",
      "2019-01-12T17:23:39.948456: step 52, loss 1.49283, acc 0.515625\n",
      "2019-01-12T17:23:40.099086: step 53, loss 1.95337, acc 0.46875\n",
      "2019-01-12T17:23:40.241670: step 54, loss 1.85071, acc 0.453125\n",
      "2019-01-12T17:23:40.405275: step 55, loss 1.71398, acc 0.5\n",
      "2019-01-12T17:23:40.560819: step 56, loss 1.44498, acc 0.46875\n",
      "2019-01-12T17:23:40.708422: step 57, loss 1.29425, acc 0.53125\n",
      "2019-01-12T17:23:40.857091: step 58, loss 1.46544, acc 0.5625\n",
      "2019-01-12T17:23:41.030563: step 59, loss 1.44703, acc 0.5\n",
      "2019-01-12T17:23:41.194122: step 60, loss 2.03109, acc 0.390625\n",
      "2019-01-12T17:23:41.330757: step 61, loss 2.26079, acc 0.453125\n",
      "2019-01-12T17:23:41.470420: step 62, loss 2.27595, acc 0.46875\n",
      "2019-01-12T17:23:41.622977: step 63, loss 1.88959, acc 0.484375\n",
      "2019-01-12T17:23:41.780561: step 64, loss 1.73794, acc 0.5\n",
      "2019-01-12T17:23:41.952097: step 65, loss 1.66814, acc 0.546875\n",
      "2019-01-12T17:23:42.129631: step 66, loss 1.80952, acc 0.546875\n",
      "2019-01-12T17:23:42.295176: step 67, loss 1.61283, acc 0.515625\n",
      "2019-01-12T17:23:42.441784: step 68, loss 1.88587, acc 0.546875\n",
      "2019-01-12T17:23:42.582408: step 69, loss 1.83234, acc 0.5\n",
      "2019-01-12T17:23:42.730015: step 70, loss 1.64934, acc 0.59375\n",
      "2019-01-12T17:23:42.869678: step 71, loss 1.28561, acc 0.546875\n",
      "2019-01-12T17:23:43.011263: step 72, loss 1.97384, acc 0.46875\n",
      "2019-01-12T17:23:43.151953: step 73, loss 1.95782, acc 0.390625\n",
      "2019-01-12T17:23:43.293506: step 74, loss 1.79184, acc 0.46875\n",
      "2019-01-12T17:23:43.437191: step 75, loss 1.67701, acc 0.59375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-12T17:23:43.579775: step 76, loss 1.73897, acc 0.515625\n",
      "2019-01-12T17:23:43.723356: step 77, loss 1.40548, acc 0.546875\n",
      "2019-01-12T17:23:43.882964: step 78, loss 1.20005, acc 0.625\n",
      "2019-01-12T17:23:44.022597: step 79, loss 1.53002, acc 0.515625\n",
      "2019-01-12T17:23:44.171158: step 80, loss 1.69595, acc 0.53125\n",
      "2019-01-12T17:23:44.315771: step 81, loss 1.42021, acc 0.578125\n",
      "2019-01-12T17:23:44.473351: step 82, loss 1.44988, acc 0.515625\n",
      "2019-01-12T17:23:44.620956: step 83, loss 1.53933, acc 0.5625\n",
      "2019-01-12T17:23:44.792497: step 84, loss 1.64233, acc 0.609375\n",
      "2019-01-12T17:23:44.951115: step 85, loss 1.365, acc 0.515625\n",
      "2019-01-12T17:23:45.102666: step 86, loss 1.67878, acc 0.53125\n",
      "2019-01-12T17:23:45.274259: step 87, loss 1.71439, acc 0.5625\n",
      "2019-01-12T17:23:45.428794: step 88, loss 1.64934, acc 0.515625\n",
      "2019-01-12T17:23:45.568422: step 89, loss 1.61237, acc 0.5625\n",
      "2019-01-12T17:23:45.719082: step 90, loss 1.44183, acc 0.578125\n",
      "2019-01-12T17:23:45.861636: step 91, loss 2.00752, acc 0.515625\n",
      "2019-01-12T17:23:46.008244: step 92, loss 1.66675, acc 0.453125\n",
      "2019-01-12T17:23:46.171808: step 93, loss 1.50572, acc 0.484375\n",
      "2019-01-12T17:23:46.323468: step 94, loss 1.58217, acc 0.53125\n",
      "2019-01-12T17:23:46.464025: step 95, loss 1.74002, acc 0.484375\n",
      "2019-01-12T17:23:46.618613: step 96, loss 1.6783, acc 0.5625\n",
      "2019-01-12T17:23:46.758241: step 97, loss 1.57019, acc 0.53125\n",
      "2019-01-12T17:23:46.898862: step 98, loss 1.64432, acc 0.5\n",
      "2019-01-12T17:23:47.039486: step 99, loss 1.6702, acc 0.453125\n",
      "2019-01-12T17:23:47.187124: step 100, loss 1.20124, acc 0.5625\n",
      "\n",
      "Evaluation:\n",
      "2019-01-12T17:23:47.509229: step 100, loss 0.754416, acc 0.572233\n",
      "\n",
      "Saved model checkpoint to D:\\New\\ML Group\\tensorflow2\\cnn-text-classification-tf-master\\runs\\1547294011\\checkpoints\\model-100\n",
      "\n",
      "2019-01-12T17:23:47.997417: step 101, loss 1.3151, acc 0.546875\n",
      "2019-01-12T17:23:48.148015: step 102, loss 1.61315, acc 0.5625\n",
      "2019-01-12T17:23:48.294624: step 103, loss 1.27597, acc 0.578125\n",
      "2019-01-12T17:23:48.433255: step 104, loss 1.57246, acc 0.421875\n",
      "2019-01-12T17:23:48.581855: step 105, loss 1.26207, acc 0.625\n",
      "2019-01-12T17:23:48.736441: step 106, loss 1.79181, acc 0.53125\n",
      "2019-01-12T17:23:48.888036: step 107, loss 1.42545, acc 0.515625\n",
      "2019-01-12T17:23:49.029656: step 108, loss 1.25469, acc 0.59375\n",
      "2019-01-12T17:23:49.180254: step 109, loss 1.51281, acc 0.578125\n",
      "2019-01-12T17:23:49.328856: step 110, loss 1.35658, acc 0.578125\n",
      "2019-01-12T17:23:49.471475: step 111, loss 1.52314, acc 0.5\n",
      "2019-01-12T17:23:49.612098: step 112, loss 1.51319, acc 0.484375\n",
      "2019-01-12T17:23:49.773665: step 113, loss 1.63111, acc 0.46875\n",
      "2019-01-12T17:23:49.914288: step 114, loss 1.20341, acc 0.671875\n",
      "2019-01-12T17:23:50.057906: step 115, loss 1.16448, acc 0.625\n",
      "2019-01-12T17:23:50.204514: step 116, loss 2.01594, acc 0.421875\n",
      "2019-01-12T17:23:50.354113: step 117, loss 1.4469, acc 0.484375\n",
      "2019-01-12T17:23:50.494736: step 118, loss 1.73951, acc 0.453125\n",
      "2019-01-12T17:23:50.632370: step 119, loss 1.37202, acc 0.53125\n",
      "2019-01-12T17:23:50.780971: step 120, loss 0.867533, acc 0.65625\n",
      "2019-01-12T17:23:50.944533: step 121, loss 1.67045, acc 0.59375\n",
      "2019-01-12T17:23:51.085157: step 122, loss 1.30609, acc 0.609375\n",
      "2019-01-12T17:23:51.227777: step 123, loss 1.52016, acc 0.515625\n",
      "2019-01-12T17:23:51.378374: step 124, loss 1.20528, acc 0.53125\n",
      "2019-01-12T17:23:51.516005: step 125, loss 1.24391, acc 0.578125\n",
      "2019-01-12T17:23:51.655632: step 126, loss 1.71959, acc 0.46875\n",
      "2019-01-12T17:23:51.808224: step 127, loss 1.66763, acc 0.484375\n",
      "2019-01-12T17:23:51.953832: step 128, loss 1.02485, acc 0.5625\n",
      "2019-01-12T17:23:52.089471: step 129, loss 1.09362, acc 0.5625\n",
      "2019-01-12T17:23:52.235082: step 130, loss 1.40872, acc 0.46875\n",
      "2019-01-12T17:23:52.379694: step 131, loss 1.4433, acc 0.421875\n",
      "2019-01-12T17:23:52.522314: step 132, loss 0.983081, acc 0.703125\n",
      "2019-01-12T17:23:52.668921: step 133, loss 1.39313, acc 0.5625\n",
      "2019-01-12T17:23:52.823506: step 134, loss 1.72687, acc 0.4375\n",
      "2019-01-12T17:23:52.967123: step 135, loss 1.33715, acc 0.5625\n",
      "2019-01-12T17:23:53.108743: step 136, loss 1.26248, acc 0.5625\n",
      "2019-01-12T17:23:53.255353: step 137, loss 1.36939, acc 0.640625\n",
      "2019-01-12T17:23:53.394979: step 138, loss 1.04991, acc 0.609375\n",
      "2019-01-12T17:23:53.533606: step 139, loss 1.21053, acc 0.546875\n",
      "2019-01-12T17:23:53.683208: step 140, loss 0.944406, acc 0.546875\n",
      "2019-01-12T17:23:53.827821: step 141, loss 1.20756, acc 0.59375\n",
      "2019-01-12T17:23:53.966451: step 142, loss 1.09677, acc 0.546875\n",
      "2019-01-12T17:23:54.113056: step 143, loss 1.58831, acc 0.5625\n",
      "2019-01-12T17:23:54.271631: step 144, loss 1.30426, acc 0.5625\n",
      "2019-01-12T17:23:54.446165: step 145, loss 1.65329, acc 0.453125\n",
      "2019-01-12T17:23:54.583797: step 146, loss 1.18858, acc 0.546875\n",
      "2019-01-12T17:23:54.735392: step 147, loss 1.10621, acc 0.640625\n",
      "2019-01-12T17:23:54.880005: step 148, loss 1.40377, acc 0.546875\n",
      "2019-01-12T17:23:55.029658: step 149, loss 1.09175, acc 0.5625\n",
      "2019-01-12T17:23:55.180204: step 150, loss 1.18287, acc 0.566667\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 1, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "# FLAGS._parse_flags()\n",
    "# print(\"\\nParameters:\")\n",
    "# for attr, value in sorted(FLAGS.__flags.items()):\n",
    "#     print(\"{}={}\".format(attr.upper(), value))\n",
    "# print(\"\")\n",
    "\n",
    "def preprocess():\n",
    "    # Data Preparation\n",
    "    # ==================================================\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    return x_train, y_train, vocab_processor, x_dev, y_dev\n",
    "\n",
    "def train(x_train, y_train, vocab_processor, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "          log_device_placement=FLAGS.log_device_placement)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "            # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Dev summaries\n",
    "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "            # Write vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            def train_step(x_batch, y_batch):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "\n",
    "            # Generate batches\n",
    "            batches = data_helpers.batch_iter(\n",
    "                list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "def main(argv=None):\n",
    "    x_train, y_train, vocab_processor, x_dev, y_dev = preprocess()\n",
    "    train(x_train, y_train, vocab_processor, x_dev, y_dev)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
